{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문 요약 \n",
    "* Bert로 Sentence Embedding을 만드는 방법을 소개\n",
    "\n",
    "* 학습 데이터 유형(Classification, Regression) 별 Sentence Embedding를 생성하는 구조 소개\n",
    "\n",
    "* 다른 Sentence Embedding 모델(Glove, Universal Sentence Encoder)보다 성능이 우수함을 입증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 유형에 맞는 Sentence Embedding 구조 소개"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regeression 데이터에 맞는 Sentence Embedding 구조\n",
    "> 모델을 STS로 Finetuning 하는 방법이 아님, STS 데이터를 어떻게 Sentence Embedding에 넣을 것인지에 대한 내용임\n",
    "\n",
    "* STS Task는 문장의 유사도(0~5) 범위를 output으로 산출함. \n",
    "\n",
    "* 이러한 데이터를 기반으로 학습하기 위해선 아래의 구조가 필요\n",
    "\n",
    "\b<img src='img/SBERT_Siamese_Network.png' alt='siamese' width='300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformers로 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel,ElectraTokenizer\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KorSTS 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한 남자가 큰 플루트를 연주하고 있다.</td>\n",
       "      <td>남자가 플루트를 연주하고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>한 남자가 피자에 치즈를 뿌려놓고 있다.</td>\n",
       "      <td>한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sen1                          sen2  score\n",
       "0           비행기가 이륙하고 있다.                 비행기가 이륙하고 있다.  5.000\n",
       "1   한 남자가 큰 플루트를 연주하고 있다.             남자가 플루트를 연주하고 있다.  3.800\n",
       "2  한 남자가 피자에 치즈를 뿌려놓고 있다.  한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.  3.800"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('data/KorSTS/sts-train.tsv') as f :\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip('\\n').split('\\t') for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:],columns=lst[:1])\n",
    "data = data[['sentence1','sentence2','score']]\n",
    "data.columns = ['sen1','sen2','score']\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Huggingface Dataset으로 데이터 전환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "\n",
    "\n",
    "class modelWithPooling(nn.Module) : \n",
    "    def __init__(self, model, pooling_type='mean') -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model # base model\n",
    "        self.pooling_type = pooling_type # pooling type 선정\n",
    "\n",
    "    # def forward(self,**kwargs) :\n",
    "    def forward(self,**kwargs) :\n",
    "        features = self.model(**kwargs)\n",
    "        attention_mask = kwargs['attention_mask'] # [batch_size, src_token, embed_size]\n",
    "        last_hidden_state = features['last_hidden_state'] # [batch_size, src_token, embed_size]\n",
    "\n",
    "        output_vectors = []\n",
    "        if self.pooling_type == 'cls':\n",
    "            '''\n",
    "            [cls] 부분만 추출\n",
    "            '''\n",
    "\n",
    "            cls_token = last_hidden_state[:, 0] # [batch_size, embed_size]\n",
    "            result = cls_token\n",
    "\n",
    "        if self.pooling_type == 'max':\n",
    "            '''\n",
    "            문장 내 토큰 중 가장 값이 큰 token만 추출\n",
    "            '''\n",
    "\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            last_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "            max_over_time = torch.max(last_hidden_state, 1)[0]\n",
    "            result = max_over_time\n",
    "\n",
    "        if self.pooling_type == 'mean':\n",
    "            '''\n",
    "            문장 내 토큰을 합한 뒤 평균\n",
    "            '''\n",
    "            # padding 부분 찾기\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() # [batch_size, src_token, embed_size]\n",
    "            # padding인 경우 0 아닌 경우 1곱한 뒤 총합\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1) # [batch_size, embed_size]\n",
    "\n",
    "            # 평균 내기위한 token 개수 \n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "            result = sum_embeddings / sum_mask\n",
    "\n",
    "        #  input.shape : [batch_size, src_token, embed_size] => output.shape : [batch_size, embed_size]\n",
    "        return {'sentence_embedding' : result}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_pooling = modelWithPooling(model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def smart_batching_collate(batch):\n",
    "        \"\"\"\n",
    "        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n",
    "        Here, batch is a list of tuples: [(tokens, label), ...]\n",
    "\n",
    "        :param batch:\n",
    "            a batch from a SmartBatchingDataset\n",
    "        :return:\n",
    "            a batch of tensors for the model\n",
    "        \"\"\"\n",
    "        \n",
    "        num_texts = 2\n",
    "        text_lst1 = []\n",
    "        text_lst2 = []\n",
    "        labels = []\n",
    "\n",
    "        for example in batch:\n",
    "            for k,v in example.items():\n",
    "                if k == 'sen1' :\n",
    "                    text_lst1.append(v)\n",
    "                if k == 'sen2' :\n",
    "                    text_lst2.append(v)\n",
    "                if k == 'score' :\n",
    "                    labels.append(float(v))\n",
    "\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "\n",
    "        sentence_features = []\n",
    "        for items in [text_lst1,text_lst2]:\n",
    "            tokenized = tokenizer(items,return_tensors='pt',truncation=True,padding=True)\n",
    "            sentence_features.append(tokenized)\n",
    "\n",
    "        return sentence_features,labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data 구조 파악 및 Cosine Similarity Loss Function 이해가 필요한 경우 아래 코드를 활용할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# featured_data = next(iter(DataLoader(train_data_set,batch_size=4,collate_fn=smart_batching_collate)))\n",
    "\n",
    "# feature, labels = featured_data\n",
    "\n",
    "# pprint(feature)\n",
    "\n",
    "\n",
    "# embeddings = [model_with_pooling(**input_data)['sentence_embedding'] for input_data in feature]\n",
    "\n",
    "# cos_score_transformation=nn.Identity()\n",
    "\n",
    "# # cosine similiarty\n",
    "# output = cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1]))\n",
    "\n",
    "# # Loss function 정의\n",
    "# mse = nn.MSELoss()\n",
    "\n",
    "# # normalization\n",
    "# labels = (labels/5)\n",
    "\n",
    "# # Loss 계산\n",
    "# loss = mse(output, labels.view(-1))\n",
    "\n",
    "# print(torch.cosine_similarity(embeddings[0], embeddings[1]))\n",
    "# print(cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, TrainerCallback,Trainer,DataCollatorForLanguageModeling\n",
    "\n",
    "class Classification_model_trainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.MSE = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        ##########################\n",
    "\n",
    "        # cosine Similarity Loss 구현\n",
    "\n",
    "        ##########################            \n",
    "\n",
    "\n",
    "        features, score = inputs\n",
    "\n",
    "\n",
    "        cos_score_transformation=nn.Identity()\n",
    "        \n",
    "        # Sentence 1, Sentence 2에 대한 Embedding\n",
    "        embeddings = [model_with_pooling(**input_data)['sentence_embedding'] for input_data in features]\n",
    "\n",
    "        # Sentence 1, Sentence 2에 대한 Cosine Similarity 계산\n",
    "        outputs = cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1]))\n",
    "\n",
    "        # label score Normalization\n",
    "        score = score / 5 # 1 ~ 5 => 0 ~ 1\n",
    "        \n",
    "        loss = self.MSE(outputs, score.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "trainer = Classification_model_trainer(model = model_with_pooling, train_dataset=train_data_set,args=training_args,data_collator=smart_batching_collate)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 데이터에 적합한 학습 구조 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/SBERT_SoftmaxLoss.png' alt='siamese' width='300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KorNLI 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2     gold_label\n",
       "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.        neutral\n",
       "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.  contradiction\n",
       "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.     entailment"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('data/KorNLI/snli_1.0_train.ko.tsv') as f :\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip('\\n').split('\\t') for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:],columns=lst[:1])\n",
    "data.columns = ['sen1','sen2','gold_label']\n",
    "data.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인코딩 gold_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2  gold_label\n",
       "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.           2\n",
       "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.           0\n",
       "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.           1"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "data['gold_label'] = data['gold_label'].replace(label2int).values\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def smart_batching_collate(batch):\n",
    "        \"\"\"\n",
    "        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n",
    "        Here, batch is a list of tuples: [(tokens, label), ...]\n",
    "\n",
    "        :param batch:\n",
    "            a batch from a SmartBatchingDataset\n",
    "        :return:\n",
    "            a batch of tensors for the model\n",
    "        \"\"\"\n",
    "        \n",
    "        num_texts = 2\n",
    "        text_lst1 = []\n",
    "        text_lst2 = []\n",
    "        labels = []\n",
    "\n",
    "        for example in batch:\n",
    "            for k,v in example.items():\n",
    "                if k == 'sen1' :\n",
    "                    text_lst1.append(v)\n",
    "                if k == 'sen2' :\n",
    "                    text_lst2.append(v)\n",
    "                if k == 'gold_label' :\n",
    "                    labels.append(int(v))\n",
    "\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "\n",
    "        sentence_features = []\n",
    "        for items in [text_lst1,text_lst2]:\n",
    "            tokenized = tokenizer(items,return_tensors='pt',truncation=True,padding=True)\n",
    "            sentence_features.append(tokenized)\n",
    "\n",
    "        return sentence_features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_set = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_model_trainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        sentence_embedding_dimension = self.model.model.config.hidden_size\n",
    "        num_vectors_concatenated = 3\n",
    "\n",
    "        self.classifier = nn.Linear(num_vectors_concatenated * sentence_embedding_dimension, 3)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        ##########################\n",
    "\n",
    "        #  SoftmaxLoss 구현\n",
    "\n",
    "        ##########################\n",
    "        \n",
    "\n",
    "        features, score = inputs\n",
    "\n",
    "        embeddings = [model_with_pooling(**input_data)['sentence_embedding'] for input_data in features]\n",
    "\n",
    "        rep_a, rep_b = embeddings\n",
    "\n",
    "        vectors_concat = []\n",
    "        vectors_concat.append(rep_a)\n",
    "        vectors_concat.append(rep_b)\n",
    "        vectors_concat.append(torch.abs(rep_a - rep_b))\n",
    "\n",
    "        features = torch.cat(vectors_concat, 1)\n",
    "        \n",
    "\n",
    "        outputs = self.classifier(features)\n",
    "\n",
    "\n",
    "\n",
    "        if score is not None:\n",
    "            loss = self.loss_fct(outputs, score .view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return embeddings, outputs\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "trainer = Regression_model_trainer(model = model_with_pooling, train_dataset=train_data_set,args=training_args,data_collator=smart_batching_collate)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
