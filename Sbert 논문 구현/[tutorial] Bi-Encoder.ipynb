{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Bert 논문 핵심 요약\n",
    "\n",
    "- Bert 모델을 활용해 Sentence Embedding을 산출하는 모델 소개\n",
    "\n",
    "- 데이터 유형(`Categorical Data`, `Numerical Data`)에 맞게 언어 모델을 Sentence Bert로 Fine-tuning하는 방법 소개\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Network(샴 네트워크)\n",
    "\n",
    "- Bi eoncoder는 샴 네트워크 구조를 취함. 샴 네트워크(Siamese network)란 하나의 모델로 두 개의 output을 산출하는 구조를 의미함.\n",
    "\n",
    "- 아래 구조를 보면 Bert 모델이 두 개 사용되는 것으로 나타나지만 실제로는 하나의 Bert 모델을 활용해 개별 문장의 output을 산출하게 됨.\n",
    "\n",
    "    <img src ='../images/SBERT_Siamese_Network.png' alt='SBERT_Siamese_Network' width ='300px'/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi encoder 구현하기\n",
    "\n",
    "- 여느 Fine-tuning 방법과 같이 last_hidden_state를 활용해 Bi-encoder 구현함.\n",
    "\n",
    "- Bert 모델을 거친 문장은 문장의 Token 개수 만큼의 Embedding이 존재함. 이때 여러 개의 Embedding을 하나의 Embedding으로 통합하는 과정을 Pooling이라 하는데, 여러 개의 토큰 임베딩을 하나의 Embedding으로 변환하면 Sentence Embedding이 됨.\n",
    "\n",
    "- Pooling 방법에는 [CLS] pooling, mean pooling, max pooling이 활용 가능하나 논문에서는 mean pooling이 가장 성능이 좋아 기본 값으로 활용함. \n",
    "\n",
    "\n",
    "\n",
    "    <img src ='../images/SBERT_Architecture.png' alt='SBERT_Architecture' width ='150px'/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentece Bert 구조 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 결과 \n",
      " ['나', '##는', '어제', '맥', '##북', '##을', '샀', '##다', '.']\n",
      "\n",
      "PLM output shape => torch.Size([1, 11, 768]) \n",
      "Sbert output shape => torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    ElectraModel,\n",
    "    ElectraTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    Trainer,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "class SentenceBert(nn.Module):\n",
    "    \"\"\"\n",
    "    Sentence Bert 논문을 읽고 관련 Repo를 참고해 만든 모델입니다.\n",
    "\n",
    "    Huggingface Trainer API를 쉽게 활용할 수 있도록 모델을 일부 수정했습니다.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    - model : Huggingface에서 제공하는 BaseModel을 활용해야 합니다.\n",
    "    - tokenizer : 모델에 맞는 토크나이저를 사용해야하며 TokenizerFast를 통해 불러와야합니다.\n",
    "    - model 및 tokenizer 설정이 없는 경우 \"monologg/koelectra-base-v3-discriminator\" 를 기본 모델로 사용합니다.\n",
    "    - pooling_type : 논문에서 제시하는 Pooling 방법인 mean pooling, max pooling, CLS pooling을 지원하며 기본 설정 값은 mean입니다.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=None, pooling_type: str = \"mean\") -> None:\n",
    "        super().__init__()\n",
    "        name = \"monologg/koelectra-base-v3-discriminator\"\n",
    "        self.model = model if model else ElectraModel.from_pretrained(name)\n",
    "\n",
    "        if pooling_type in [\"mean\", \"max\", \"cls\"] and type(pooling_type) == str:\n",
    "            self.pooling_type = pooling_type\n",
    "        else:\n",
    "            raise ValueError(\"'pooling_type' only ['mean','max','cls'] possible\")\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        last_hidden_state = self.model(**kwargs)[\"last_hidden_state\"]\n",
    "\n",
    "        if self.pooling_type == \"cls\":\n",
    "            \"\"\"[cls] token을 sentence embedding으로 활용\"\"\"\n",
    "            result = last_hidden_state[:, 0]\n",
    "\n",
    "        if self.pooling_type == \"max\":\n",
    "            \"\"\"문장 내 여러 토큰 중 가장 값이 큰 token만 추출하여 sentence embedding으로 활용\"\"\"\n",
    "\n",
    "            num_of_tokens = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            last_hidden_state[num_of_tokens == 0] = -1e9\n",
    "            result = torch.max(last_hidden_state, 1)[0]\n",
    "\n",
    "        if self.pooling_type == \"mean\":\n",
    "            \"\"\"문장 내 토큰을 평균하여 sentence embedding으로 활용\"\"\"\n",
    "\n",
    "            num_of_tokens = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "\n",
    "            sum_embeddings = torch.sum(last_hidden_state * num_of_tokens, 1)\n",
    "\n",
    "            total_num_of_tokens = num_of_tokens.sum(1)\n",
    "            total_num_of_tokens = torch.clamp(total_num_of_tokens, min=1e-9)\n",
    "\n",
    "            result = sum_embeddings / total_num_of_tokens\n",
    "\n",
    "        return {\"sentence_embedding\": result}\n",
    "\n",
    "\n",
    "### Sbert 불러오기\n",
    "name = \"monologg/koelectra-base-v3-discriminator\"\n",
    "model = ElectraModel.from_pretrained(name)\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(name)\n",
    "\n",
    "sbert = SentenceBert(model=model, pooling_type=\"mean\")\n",
    "\n",
    "sen = \"나는 어제 맥북을 샀다.\"\n",
    "\n",
    "token = tokenizer(sen, return_tensors=\"pt\")\n",
    "PLM = model(**token)[\"last_hidden_state\"]\n",
    "sentence_embedding = sbert(**token)[\"sentence_embedding\"]\n",
    "\n",
    "print(\"Tokenizing 결과 \\n\", tokenizer.tokenize(sen))\n",
    "print(\"\")\n",
    "print(f\"PLM output shape => {PLM.shape} \\nSbert output shape => {sentence_embedding.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 유형 별 Sentence Bert 학습 구조\n",
    "\n",
    "- SentenceBert를 학습 시키는 방법은 학습 데이터의 유형에 따라 달라짐\n",
    "\n",
    "- `Numerical Data`를 기반으로 Sentence Bert를 학습시키는 경우 학습 구조는 다음과 같음.\n",
    "\n",
    "    <img src ='../images/SBERT_Siamese_Network.png' alt='SBERT_Siamese_Network' width ='300px'/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "- `Categorical Data`를 기반으로 Sentence Bert를 학습시키는 경우 학습 구조는 다음과 같음.\n",
    "\n",
    "    <img src ='../images/SBERT_SoftmaxLoss.png' alt='SBERT_SoftmaxLoss' width ='300px'/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data 학습 시 Sbert 구조\n",
    "\n",
    "- 논문에서는 Numerical Data 데이터로 STS 데이터를 활용함. 이 글에서는 `KorSTS` 데이터를 활용함.\n",
    "\n",
    "- STS 데이터는 문장 2개와 문장의 유사도를 표현한 값으로 구성됨.\n",
    "\n",
    "  ```python\n",
    "\n",
    "  {\n",
    "  'sen1': '비행기가 이륙하고 있다.',\n",
    "  'sen2': '비행기가 이륙하고 있다.',\n",
    "  'score': '5.000'\n",
    "  }\n",
    "\n",
    "  ```\n",
    "\n",
    "- 학습이 완료된 이후에는 학습 구조에서 Sbert를 추출하여 활용함.\n",
    "\n",
    "    <img src='../images/SBERT_Siamese_Network.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelForRegressionTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # 학습을 수행할 모델 불러오기\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        # Sentence 1, Sentence 2에 대한 Sentence Embedding 확보\n",
    "        embeddings = [self.model(**input_data)[\"sentence_embedding\"] for input_data in features]\n",
    "\n",
    "        u, v = embeddings[0], embeddings[1]\n",
    "\n",
    "        # Sentence 1, Sentence 2에 대한 Cosine Similarity 계산\n",
    "        cos_score_transformation = nn.Identity()\n",
    "        outputs = cos_score_transformation(torch.cosine_similarity(u, v))\n",
    "\n",
    "        # label score Normalization\n",
    "        answer = answer / 5  # 0 ~ 5 => 0 ~ 1\n",
    "\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorSTS Data 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한 남자가 큰 플루트를 연주하고 있다.</td>\n",
       "      <td>남자가 플루트를 연주하고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>한 남자가 피자에 치즈를 뿌려놓고 있다.</td>\n",
       "      <td>한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sen1                          sen2  score\n",
       "0           비행기가 이륙하고 있다.                 비행기가 이륙하고 있다.  5.000\n",
       "1   한 남자가 큰 플루트를 연주하고 있다.             남자가 플루트를 연주하고 있다.  3.800\n",
       "2  한 남자가 피자에 치즈를 뿌려놓고 있다.  한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.  3.800"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/KorSTS/sts-train.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data = data[[\"sentence1\", \"sentence2\", \"score\"]]\n",
    "data.columns = [\"sen1\", \"sen2\", \"score\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Datasets으로 변환\n",
    "\n",
    "- 🤗Transformers와 호환을 위해 Dataframe을 🤗dataset으로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': '비행기가 이륙하고 있다.', 'sen2': '비행기가 이륙하고 있다.', 'score': '5.000'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현\n",
    "\n",
    "- 학습 구조에 맞는 input data 생성을 위한 custom collator 제작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"score\":\n",
    "                labels.append(float(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        token = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(token)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤗Transformers Trainer를 활용해 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Option 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# 학습 구조 불러오기\n",
    "model_for_training = modelForRegressionTraining(sbert)\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data 학습 시 Sbert 구조\n",
    "\n",
    "- 논문에서는 Categorical Data로 NLI 데이터를 활용함. 이 글에서는 `KorNLI` 데이터 중 `snli_1.0_train.ko`를 활용함.\n",
    "\n",
    "- KorNLI 데이터는 문장 2개와 문장의 관계를 Label로 표현함.\n",
    "\n",
    "```python\n",
    "\n",
    "  {\n",
    "   'sen1': '그리고 그가 말했다, \"엄마, 저 왔어요.\"',\n",
    "   'sen2': '그는 학교 버스가 그를 내려주자마자 엄마에게 전화를 걸었다.',\n",
    "   'gold_label': 'neutral'\n",
    "   }\n",
    "\n",
    "```\n",
    "\n",
    "- 학습이 완료된 이후에는 학습 구조에서 Sbert를 추출하여 활용함.\n",
    "\n",
    "    <img src='../images/SBERT_SoftmaxLoss.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class modelForClassificationTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # 학습할 모델 불러오기\n",
    "        self.model = model\n",
    "\n",
    "        # 모델 embed_size\n",
    "        sentence_embedding_dimension = self.model.model.config.hidden_size\n",
    "\n",
    "        # concat 해야하는 vector 개수(U,V, |U-V|)\n",
    "        num_vectors_concatenated = 3\n",
    "\n",
    "        # embed_size * 3 => 3 차원으로 축소시키는 classifier\n",
    "        self.classifier = nn.Linear(num_vectors_concatenated * sentence_embedding_dimension, 3)\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        # Sentence Embedding 생성\n",
    "        embeddings = [self.model(**input_data)[\"sentence_embedding\"] for input_data in features]\n",
    "\n",
    "        u, v = embeddings\n",
    "\n",
    "        # U,V, |U-V| vector 병합\n",
    "        vectors_concat = []\n",
    "        vectors_concat.append(u)\n",
    "        vectors_concat.append(v)\n",
    "        vectors_concat.append(torch.abs(u - v))\n",
    "        features = torch.cat(vectors_concat, 1)\n",
    "\n",
    "        # 병합한 vector 차원 축소\n",
    "        outputs = self.classifier(features)\n",
    "\n",
    "        # Loss 계산\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorNLI Data 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2     gold_label\n",
       "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.        neutral\n",
       "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.  contradiction\n",
       "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.     entailment"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"../data/KorNLI/snli_1.0_train.ko.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data.columns = [\"sen1\", \"sen2\", \"gold_label\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gold_label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2  gold_label\n",
       "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.           2\n",
       "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.           0\n",
       "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.           1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "data[\"gold_label\"] = data[\"gold_label\"].replace(label2int).values\n",
    "\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Dataset으로 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': '말을 탄 사람이 고장난 비행기 위로 뛰어오른다.',\n",
       " 'sen2': '한 사람이 경쟁을 위해 말을 훈련시키고 있다.',\n",
       " 'gold_label': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"gold_label\":\n",
    "                labels.append(int(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        tokenized = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(tokenized)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤗Transformers Trainer를 활용해 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Option 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# 학습 구조 불러오기\n",
    "model_for_training = modelForClassificationTraining(sbert)\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
