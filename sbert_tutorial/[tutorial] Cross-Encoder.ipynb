{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross encoder 구조\n",
    "\n",
    "- 이 글은 Cross Encoder의 구조를 소개하고 학습하는 방법을 설명함. \n",
    "\n",
    "- Cross encoder는 Pretrained Model에 Classification layer를 쌓은 구조임. \n",
    "\n",
    "- Cross Encoder는 🤗Transforemers의 Sequenceclassification model을 불러와 쉽게 구현할 수 있음.\n",
    "\n",
    "- Sentence_transformers 라이브러리에서 구현된 Cross_Encoder 또한 내부에 SequenceClassification를 기반으로 구성됨.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 불러오기\n",
    "\n",
    "* Cross Encoder는 Pretrained Model에 Classification Head를 연결한 모델임.\n",
    "\n",
    "    <img src='../images/cross_encoder.png' alt='cross_encoder' width='500px'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ClassificationHead\n",
    "\n",
    "- Classification 상세 구조는 dense_layer => gelu => output_pojection_layer로 되어있음.\n",
    "\n",
    "    <img src='../images/classification_head.png' alt='classification_head' width='500px'>\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "- Pre-trained Model의 output인 last_hidden_state 중 [CLS] 토큰만을 Classification Head의 input data로 활용\n",
    "\n",
    "- 수많은 토큰 embedding이 있지만 그 중 [CLS] 토큰이 두 문장의 관계를 요약한 embedding이라 판단하므로 이를 활용함.  \n",
    "\n",
    "- 이처럼 여러 정보를 하나로 치환하는 방법을 pooling이라 함. pooling 관련해서는 [Why it called pooler?](https://github.com/google-research/bert/issues/1102)를 참고.\n",
    "\n",
    "- pooling 된 embedding은 ReLU(또는 Tanh) 함수를 거친 다음 projection layer를 통해 라벨 크기에 맞는 차원으로 축소함.\n",
    "\n",
    "- Regression 데이터로 학습하는 경우 라벨 개수(N)은 1로 설정해야하며, Classification 데이터로 학습하는 경우 라벨 개수에 맞게 N을 설정해야함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizerFast, TrainingArguments, Trainer\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from datasets import Dataset\n",
    "from torch import Tensor, nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout\n",
    "            if config.classifier_dropout is not None\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.gelu = nn.functional.gelu\n",
    "\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "\n",
    "        # [batch, embed_size] => [batch, num_labels]\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # [CLS] 토큰 추출\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # label 개수만큼 차원 축소 [batch, embed_size] => [batch, num_labels]\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEncoder 구조\n",
    "\n",
    "- 아래의 CrossEncoder 구조는 🤗Transforemers의 Sequenceclassification 내부 코드를 기반으로 작성했으며, 이해하기 쉽도록 코드 일부를 변경하였음.\n",
    "\n",
    "- Cross_Encoder의 Output은 모델 학습 시 Loss와 Logits을 반환하고 평가 및 활용 시에는 Logits만 반환함.\n",
    "\n",
    "- 학습에 데이터에 따라 Loss_function이 달라짐. 학습 유형이 Regression일 때 MSE, Classfication일 때 Cross-Enctropy를 활용함.\n",
    "\n",
    "- 학습 유형에 따라 Loss Function이 달라지는 이유가 궁금한 경우 다음을 참고\n",
    "    - [In which cases is the cross-entropy preferred over the mean squared error?](https://stackoverflow.com/questions/36515202/in-which-cases-is-the-cross-entropy-preferred-over-the-mean-squared-error)\n",
    "    \n",
    "    - [What is the different between MSE error and Cross-entropy error in NN](https://susanqq.github.io/tmp_post/2017-09-05-crossentropyvsmes/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, model, num_labels) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.config.num_labels = num_labels\n",
    "        self.classifier = classificationHead(self.model.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        discriminator_hidden_states = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Last-hidden-state 추출\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "\n",
    "        # Last-hidden-state를 classificationHead의 입력 데이터로 활용\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.model.config.num_labels == 1:\n",
    "                # Regression Model은 MSE Loss 활용\n",
    "                loss_fct = MSELoss()\n",
    "            else:\n",
    "                # classification Model은 Cross entropy 활용\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, 3), labels.view(-1))\n",
    "            return {\"loss\": loss, \"logit\": logits}\n",
    "        else:\n",
    "            return {\"logit\": logits}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Encoder 학습하기\n",
    "* 이 글에선 Numerical Data를 활용해 Cross Encoder를 학습하는 방법만을 소개함."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorSTS Data 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한 남자가 큰 플루트를 연주하고 있다.</td>\n",
       "      <td>남자가 플루트를 연주하고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>한 남자가 피자에 치즈를 뿌려놓고 있다.</td>\n",
       "      <td>한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sen1                          sen2  score\n",
       "0           비행기가 이륙하고 있다.                 비행기가 이륙하고 있다.  5.000\n",
       "1   한 남자가 큰 플루트를 연주하고 있다.             남자가 플루트를 연주하고 있다.  3.800\n",
       "2  한 남자가 피자에 치즈를 뿌려놓고 있다.  한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.  3.800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"../data/KorSTS/sts-train.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data = data[[\"sentence1\", \"sentence2\", \"score\"]]\n",
    "data.columns = [\"sen1\", \"sen2\", \"score\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Datasets으로 변환\n",
    "\n",
    "- 🤗Transformers와 호환을 위해 Dataframe을 🤗dataset으로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': '비행기가 이륙하고 있다.', 'sen2': '비행기가 이륙하고 있다.', 'score': '5.000'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현\n",
    "\n",
    "- 학습 구조에 맞는 input data 생성을 위한 custom collator 제작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"score\":\n",
    "                labels.append(float(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        token = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(token)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤗Transformers Trainer를 활용해 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Option 설정\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    ")\n",
    "\n",
    "# 모델 불러오기\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "cross_encoder = CrossEncoder(model, num_labels=1) \n",
    "\n",
    "# Huggingface의 ElectraForSequenceClassification을 cross_encoder로 활용가능\n",
    "# from transformers import ElectraForSequenceClassification\n",
    "# cross_encoder = ElectraForSequenceClassification.from_pretrained('model/disc_book_final',num_labels=3)\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=cross_encoder,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
