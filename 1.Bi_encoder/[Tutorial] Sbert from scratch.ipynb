{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문 요약\n",
    "\n",
    "- Bert로 Sentence Embedding을 만드는 방법을 소개\n",
    "\n",
    "- 학습 데이터 유형(Classification, Regression) 별 Sentence Embedding를 생성하는 구조 소개\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 유형에 맞는 Sentence Embedding 구조 소개\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformers로 모델 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Model 만들기\n",
    "\n",
    "<img src ='../img/SBERT_Architecture.png' alt='SBERT_Architecture' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class modelWithPooling(nn.Module):\n",
    "    def __init__(self, model, pooling_type=\"mean\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model  # base model ex)BertModel, ElectraModel ...\n",
    "        self.pooling_type = pooling_type  # pooling type 선정\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        features = self.model(**kwargs)\n",
    "        # [batch_size, src_token, embed_size]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "\n",
    "        last_hidden_state = features[\"last_hidden_state\"]\n",
    "\n",
    "        if self.pooling_type == \"cls\":\n",
    "            \"\"\"\n",
    "            [cls] 부분만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            cls_token = last_hidden_state[:, 0]  # [batch_size, embed_size]\n",
    "            result = cls_token\n",
    "\n",
    "        if self.pooling_type == \"max\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰 중 가장 값이 큰 token만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # Set padding tokens to large negative value\n",
    "            last_hidden_state[input_mask_expanded == 0] = -1e9\n",
    "            max_over_time = torch.max(last_hidden_state, 1)[0]\n",
    "            result = max_over_time\n",
    "\n",
    "        if self.pooling_type == \"mean\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰을 합한 뒤 평균\n",
    "            \"\"\"\n",
    "            # padding 부분 찾기 = [batch_size, src_token, embed_size]\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # padding인 경우 0 아닌 경우 1곱한 뒤 총합 = [batch_size, embed_size]\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "            # 평균 내기위한 token 개수\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "            result = sum_embeddings / sum_mask\n",
    "\n",
    "        #  input.shape : [batch_size, src_token, embed_size] => output.shape : [batch_size, embed_size]\n",
    "        return {\"sentence_embedding\": result}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"score\":\n",
    "                labels.append(float(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        tokenized = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(tokenized)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regeression 데이터에 맞는 Sentence Embedding 구조\n",
    "\n",
    "> 모델을 STS로 Finetuning 하는 방법이 아님, STS 데이터를 어떻게 Sentence Embedding에 넣을 것인지에 대한 내용임\n",
    "\n",
    "- STS Task는 문장의 유사도(0~5) 범위를 output으로 산출함.\n",
    "\n",
    "- 이러한 데이터를 기반으로 학습하기 위해선 아래의 구조가 필요\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Regression Training\n",
    "\n",
    "<img src='../img/SBERT_Siamese_Network.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class modelForRegressionTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = modelWithPooling(model)\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        cos_score_transformation = nn.Identity()\n",
    "\n",
    "        # Sentence 1, Sentence 2에 대한 Embedding\n",
    "        embeddings = [\n",
    "            self.model(**input_data)[\"sentence_embedding\"] for input_data in features\n",
    "        ]\n",
    "\n",
    "        # Sentence 1, Sentence 2에 대한 Cosine Similarity 계산\n",
    "        outputs = cos_score_transformation(\n",
    "            torch.cosine_similarity(embeddings[0], embeddings[1])\n",
    "        )\n",
    "\n",
    "        # label score Normalization\n",
    "\n",
    "        answer = answer / 5  # 1 ~ 5 => 0 ~ 1\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "model_for_training = modelForRegressionTraining(model=model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression 유형 Data 불러오기(KorSTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한 남자가 큰 플루트를 연주하고 있다.</td>\n",
       "      <td>남자가 플루트를 연주하고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>한 남자가 피자에 치즈를 뿌려놓고 있다.</td>\n",
       "      <td>한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sen1                          sen2  score\n",
       "0           비행기가 이륙하고 있다.                 비행기가 이륙하고 있다.  5.000\n",
       "1   한 남자가 큰 플루트를 연주하고 있다.             남자가 플루트를 연주하고 있다.  3.800\n",
       "2  한 남자가 피자에 치즈를 뿌려놓고 있다.  한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.  3.800"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/KorSTS/sts-train.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data = data[[\"sentence1\", \"sentence2\", \"score\"]]\n",
    "data.columns = [\"sen1\", \"sen2\", \"score\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Datasets으로 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': '비행기가 이륙하고 있다.', 'sen2': '비행기가 이륙하고 있다.', 'score': '5.000'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 데이터에 적합한 학습 구조\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/SBERT_SoftmaxLoss.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Classification Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class modelForClassificationTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = modelWithPooling(model)\n",
    "\n",
    "        sentence_embedding_dimension = self.model.model.config.hidden_size\n",
    "        num_vectors_concatenated = 3\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            num_vectors_concatenated * sentence_embedding_dimension, 3\n",
    "        )\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        embeddings = [\n",
    "            self.model(**input_data)[\"sentence_embedding\"] for input_data in features\n",
    "        ]\n",
    "\n",
    "        rep_a, rep_b = embeddings\n",
    "\n",
    "        vectors_concat = []\n",
    "        vectors_concat.append(rep_a)\n",
    "        vectors_concat.append(rep_b)\n",
    "        vectors_concat.append(torch.abs(rep_a - rep_b))\n",
    "\n",
    "        features = torch.cat(vectors_concat, 1)\n",
    "\n",
    "        outputs = self.classifier(features)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "model_for_training = modelForClassificationTraining(model=model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 유형 Data 불러오기(KorNLI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2     gold_label\n",
       "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.        neutral\n",
       "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.  contradiction\n",
       "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.     entailment"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/KorNLI/snli_1.0_train.ko.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data.columns = [\"sen1\", \"sen2\", \"gold_label\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gold_label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2  gold_label\n",
       "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.           2\n",
       "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.           0\n",
       "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.           1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "data[\"gold_label\"] = data[\"gold_label\"].replace(label2int).values\n",
    "\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Dataset으로 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': '말을 탄 사람이 고장난 비행기 위로 뛰어오른다.',\n",
       " 'sen2': '한 사람이 경쟁을 위해 말을 훈련시키고 있다.',\n",
       " 'gold_label': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"gold_label\":\n",
    "                labels.append(int(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        tokenized = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(tokenized)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 550152\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 275076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed40808742df4a9b98d187b8eb2cdb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1313, 'learning_rate': 4.999818232052233e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1084, 'learning_rate': 4.9996364641044654e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1246, 'learning_rate': 4.9994546961566984e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1384, 'learning_rate': 4.999272928208932e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/IR-Ranking-project/[Tutorial] Sbert from scratch.ipynb 셀 31\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_trainer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     remove_unused_columns\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel_for_training,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_data_set,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     data_collator\u001b[39m=\u001b[39msmart_batching_collate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/IR-Ranking-project/%5BTutorial%5D%20Sbert%20from%20scratch.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1408\u001b[0m )\n\u001b[0;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1414\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/trainer.py:1718\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1716\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m scale_before \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m scale_after\n\u001b[1;32m   1717\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1718\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1720\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[1;32m   1721\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/optimization.py:362\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    360\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[1;32m    361\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m--> 362\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39;49msqrt()\u001b[39m.\u001b[39;49madd_(group[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    364\u001b[0m step_size \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mcorrect_bias\u001b[39m\u001b[39m\"\u001b[39m]:  \u001b[39m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Model 내 Encode 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class modelWithPooling(nn.Module):\n",
    "    def __init__(self, model, pooling_type=\"mean\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model  # base model ex)BertModel, ElectraModel ...\n",
    "        self.pooling_type = pooling_type  # pooling type 선정\n",
    "        self.tokenizer = None\n",
    "\n",
    "    #### Encoder 구현\n",
    "    def encode(self, items: list, tokenizer=None, batch_size: int = 16):\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            from transformers import AutoTokenizer\n",
    "\n",
    "            print(f'Loading Tokenizer : \"{self.model.config._name_or_path}\"')\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model.config._name_or_path\n",
    "            )\n",
    "\n",
    "        def default_collater(items):\n",
    "            token = self.tokenizer(\n",
    "                items, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\"sen\": items, \"token\": token}\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=items, batch_size=batch_size, collate_fn=default_collater\n",
    "        )\n",
    "\n",
    "        output_lst = []\n",
    "        sen_lst = []\n",
    "        for data in data_loader:\n",
    "            sen = data.pop(\"sen\")\n",
    "            sen_lst += sen\n",
    "            token = data.pop(\"token\")\n",
    "            outputs = self.forward(**token)[\"sentence_embedding\"]\n",
    "            output_lst.append(outputs)\n",
    "\n",
    "        return {\"sen\": sen_lst, \"sentence_embedding\": torch.cat(output_lst)}\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        features = self.model(**kwargs)\n",
    "        # [batch_size, src_token, embed_size]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "\n",
    "        last_hidden_state = features[\"last_hidden_state\"]\n",
    "\n",
    "        if self.pooling_type == \"cls\":\n",
    "            \"\"\"\n",
    "            [cls] 부분만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            cls_token = last_hidden_state[:, 0]  # [batch_size, embed_size]\n",
    "            result = cls_token\n",
    "\n",
    "        if self.pooling_type == \"max\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰 중 가장 값이 큰 token만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # Set padding tokens to large negative value\n",
    "            last_hidden_state[input_mask_expanded == 0] = -1e9\n",
    "            max_over_time = torch.max(last_hidden_state, 1)[0]\n",
    "            result = max_over_time\n",
    "\n",
    "        if self.pooling_type == \"mean\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰을 합한 뒤 평균\n",
    "            \"\"\"\n",
    "            # padding 부분 찾기 = [batch_size, src_token, embed_size]\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # padding인 경우 0 아닌 경우 1곱한 뒤 총합 = [batch_size, embed_size]\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "            # 평균 내기위한 token 개수\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "            result = sum_embeddings / sum_mask\n",
    "\n",
    "        #  input.shape : [batch_size, src_token, embed_size] => output.shape : [batch_size, embed_size]\n",
    "        return {\"sentence_embedding\": result}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
