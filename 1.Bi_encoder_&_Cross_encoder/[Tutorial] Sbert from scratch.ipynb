{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문 요약\n",
    "\n",
    "- Bert로 Sentence Embedding을 만드는 방법을 소개\n",
    "\n",
    "- 학습 데이터 유형(Classification, Regression) 별 Sentence Embedding를 생성하는 구조 소개\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 유형에 맞는 Sentence Embedding 구조 소개\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformers로 모델 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Model 만들기\n",
    "\n",
    "<img src ='../img/SBERT_Architecture.png' alt='SBERT_Architecture' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class modelWithPooling(nn.Module):\n",
    "    def __init__(self, model, pooling_type=\"mean\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model  # base model ex)BertModel, ElectraModel ...\n",
    "        self.pooling_type = pooling_type  # pooling type 선정\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        features = self.model(**kwargs)\n",
    "        # [batch_size, src_token, embed_size]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "\n",
    "        last_hidden_state = features[\"last_hidden_state\"]\n",
    "\n",
    "        if self.pooling_type == \"cls\":\n",
    "            \"\"\"\n",
    "            [cls] 부분만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            cls_token = last_hidden_state[:, 0]  # [batch_size, embed_size]\n",
    "            result = cls_token\n",
    "\n",
    "        if self.pooling_type == \"max\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰 중 가장 값이 큰 token만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # Set padding tokens to large negative value\n",
    "            last_hidden_state[input_mask_expanded == 0] = -1e9\n",
    "            max_over_time = torch.max(last_hidden_state, 1)[0]\n",
    "            result = max_over_time\n",
    "\n",
    "        if self.pooling_type == \"mean\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰을 합한 뒤 평균\n",
    "            \"\"\"\n",
    "            # padding 부분 찾기 = [batch_size, src_token, embed_size]\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # padding인 경우 0 아닌 경우 1곱한 뒤 총합 = [batch_size, embed_size]\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "            # 평균 내기위한 token 개수\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "            result = sum_embeddings / sum_mask\n",
    "\n",
    "        #  input.shape : [batch_size, src_token, embed_size] => output.shape : [batch_size, embed_size]\n",
    "        return {\"sentence_embedding\": result}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regeression 데이터에 맞는 Sentence Embedding 구조\n",
    "\n",
    "> 모델을 STS로 Finetuning 하는 방법이 아님, STS 데이터를 어떻게 Sentence Embedding에 넣을 것인지에 대한 내용임\n",
    "\n",
    "- STS Task는 문장의 유사도(0~5) 범위를 output으로 산출함.\n",
    "\n",
    "- 이러한 데이터를 기반으로 학습하기 위해선 아래의 구조가 필요\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Regression Training\n",
    "\n",
    "<img src='../img/SBERT_Siamese_Network.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class modelForRegressionTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # 학습을 수행할 모델 불러오기\n",
    "        self.model = modelWithPooling(model)\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        # Sentence 1, Sentence 2에 대한 Embedding\n",
    "        embeddings = [self.model(**input_data)[\"sentence_embedding\"] for input_data in features]\n",
    "\n",
    "        # Sentence 1, Sentence 2에 대한 Cosine Similarity 계산\n",
    "        cos_score_transformation = nn.Identity()\n",
    "        outputs = cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1]))\n",
    "\n",
    "        # label score Normalization\n",
    "        answer = answer / 5  # 0 ~ 5 => 0 ~ 1\n",
    "\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression 유형 Data 불러오기(KorSTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>비행기가 이륙하고 있다.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한 남자가 큰 플루트를 연주하고 있다.</td>\n",
       "      <td>남자가 플루트를 연주하고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>한 남자가 피자에 치즈를 뿌려놓고 있다.</td>\n",
       "      <td>한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sen1                          sen2  score\n",
       "0           비행기가 이륙하고 있다.                 비행기가 이륙하고 있다.  5.000\n",
       "1   한 남자가 큰 플루트를 연주하고 있다.             남자가 플루트를 연주하고 있다.  3.800\n",
       "2  한 남자가 피자에 치즈를 뿌려놓고 있다.  한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.  3.800"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"../data/KorSTS/sts-train.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data = data[[\"sentence1\", \"sentence2\", \"score\"]]\n",
    "data.columns = [\"sen1\", \"sen2\", \"score\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Datasets으로 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': '비행기가 이륙하고 있다.', 'sen2': '비행기가 이륙하고 있다.', 'score': '5.000'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"score\":\n",
    "                labels.append(float(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        tokenized = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(tokenized)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 데이터에 적합한 학습 구조\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/SBERT_SoftmaxLoss.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Classification Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class modelForClassificationTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # 학습할 모델 불러오기\n",
    "        self.model = modelWithPooling(model)\n",
    "\n",
    "        # 모델 embed_size\n",
    "        sentence_embedding_dimension = self.model.model.config.hidden_size\n",
    "\n",
    "        # concat 해야하는 vector 개수(U,V, |U-V|)\n",
    "        num_vectors_concatenated = 3\n",
    "\n",
    "        # embed_size * 3 => 3 차원으로 축소시키는 classifier\n",
    "        self.classifier = nn.Linear(num_vectors_concatenated * sentence_embedding_dimension, 3)\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        \"\"\"\n",
    "        샴 네트워크는 하나의 모델로 두 개의 output을 산출하는 구조임.\n",
    "        하나의 모델을 사용하지만 각각 출력하므로 Input 데이터 상호 간 영향을 줄 수 없게 됨\n",
    "        반면 Cross encoder는 이와 반대로 두 개의 문장을 묶어 하나의 Input 데이터로 만든 뒤\n",
    "        모델 내부에서 상호간 유사성을 파악하는 구조임.\n",
    "        \"\"\"\n",
    "\n",
    "        # 개별 데이터 생성\n",
    "        embeddings = [self.model(**input_data)[\"sentence_embedding\"] for input_data in features]\n",
    "\n",
    "        rep_a, rep_b = embeddings\n",
    "\n",
    "        # U,V, |U-V| vector 병합\n",
    "        vectors_concat = []\n",
    "        vectors_concat.append(rep_a)\n",
    "        vectors_concat.append(rep_b)\n",
    "        vectors_concat.append(torch.abs(rep_a - rep_b))\n",
    "\n",
    "        features = torch.cat(vectors_concat, 1)\n",
    "\n",
    "        # 병합한 vector 차원 축소\n",
    "        outputs = self.classifier(features)\n",
    "\n",
    "        # Loss 계산\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "model_for_training = modelForClassificationTraining(model=model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 유형 Data 불러오기(KorNLI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/KorNLI/snli_1.0_train.ko.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data.columns = [\"sen1\", \"sen2\", \"gold_label\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gold_label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "data[\"gold_label\"] = data[\"gold_label\"].replace(label2int).values\n",
    "\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Dataset으로 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"gold_label\":\n",
    "                labels.append(int(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        tokenized = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(tokenized)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Model 내 Encode 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class modelWithPooling(nn.Module):\n",
    "    def __init__(self, model, pooling_type=\"mean\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model  # base model ex)BertModel, ElectraModel ...\n",
    "        self.pooling_type = pooling_type  # pooling type 선정\n",
    "        self.tokenizer = None\n",
    "\n",
    "    #### Encoder 구현\n",
    "    def encode(self, items: list, tokenizer=None, batch_size: int = 16):\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            from transformers import AutoTokenizer\n",
    "\n",
    "            print(f'Loading Tokenizer : \"{self.model.config._name_or_path}\"')\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model.config._name_or_path\n",
    "            )\n",
    "\n",
    "        def default_collater(items):\n",
    "            token = self.tokenizer(\n",
    "                items, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\"sen\": items, \"token\": token}\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=items, batch_size=batch_size, collate_fn=default_collater\n",
    "        )\n",
    "\n",
    "        output_lst = []\n",
    "        sen_lst = []\n",
    "        for data in data_loader:\n",
    "            sen = data.pop(\"sen\")\n",
    "            sen_lst += sen\n",
    "            token = data.pop(\"token\")\n",
    "            outputs = self.forward(**token)[\"sentence_embedding\"]\n",
    "            output_lst.append(outputs)\n",
    "\n",
    "        return {\"sen\": sen_lst, \"sentence_embedding\": torch.cat(output_lst)}\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        features = self.model(**kwargs)\n",
    "        # [batch_size, src_token, embed_size]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "\n",
    "        last_hidden_state = features[\"last_hidden_state\"]\n",
    "\n",
    "        if self.pooling_type == \"cls\":\n",
    "            \"\"\"\n",
    "            [cls] 부분만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            cls_token = last_hidden_state[:, 0]  # [batch_size, embed_size]\n",
    "            result = cls_token\n",
    "\n",
    "        if self.pooling_type == \"max\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰 중 가장 값이 큰 token만 추출\n",
    "            \"\"\"\n",
    "\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # Set padding tokens to large negative value\n",
    "            last_hidden_state[input_mask_expanded == 0] = -1e9\n",
    "            max_over_time = torch.max(last_hidden_state, 1)[0]\n",
    "            result = max_over_time\n",
    "\n",
    "        if self.pooling_type == \"mean\":\n",
    "            \"\"\"\n",
    "            문장 내 토큰을 합한 뒤 평균\n",
    "            \"\"\"\n",
    "            # padding 부분 찾기 = [batch_size, src_token, embed_size]\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # padding인 경우 0 아닌 경우 1곱한 뒤 총합 = [batch_size, embed_size]\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "            # 평균 내기위한 token 개수\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "            result = sum_embeddings / sum_mask\n",
    "\n",
    "        #  input.shape : [batch_size, src_token, embed_size] => output.shape : [batch_size, embed_size]\n",
    "        return {\"sentence_embedding\": result}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
