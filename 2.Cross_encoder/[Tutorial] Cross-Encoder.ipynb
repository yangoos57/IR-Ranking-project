{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Encoder를 알아야 하는 이유\n",
    "\n",
    "- 문제해결 관점에서 비슷한 분야의 문서를 분류하거나, 궁금한 내용을 찾아야 하는 경우, 욕설 문장을 비교하거나 등등.. 필요한 경우가 있음.\n",
    "\n",
    "- 문장 간 관계를 1:1 비교하는 방법임.\n",
    "\n",
    "- Q&A, STS, NLI Task 모두 Cross Encoder기반으로 문제 해결가능\n",
    "\n",
    "- 문장과 문장을 1대1로 비교해 연관성을 측정하는 방법임\n",
    "\n",
    "- STS의 경우 문장 1:1로 비교해 유사도를 0~5 범위로 output 반환\n",
    "\n",
    "- NLI의 경우도 문장을 1:1로 비교해 Entailment, Neutral, Contradiction으로 Output 반환\n",
    "\n",
    "- Q&A의 경우 Question과 DataBase 내 자료를 1:1로 비교 Question 1개, Database 내 10개의 문장이 있다고 가정할 때, (Question, corpus1), (Question, corpus2) ... (Question, corpus10)과 같이 10번을 연산 한 결과를 비교해 가장 높은 값을 채택\n",
    "\n",
    "- 문장의 유사도는 번역, 요약, 문장 생성, QA, 대화 모델링 등등 다양한 NLP 분야에서 중요하게 다뤄진다\n",
    "\n",
    "<!-- * Cross Encoder를 코드로 구현하며 코드 내부 데이터 흐름에 대한 설명을 이어나가겠음. -->\n",
    "\n",
    "> 이번 글에서는 Cross Encoder 구조 이해를 위해 Base Model 위에 Cross-encoder layer를 쌓고 이를 학습하는 방법을 설명함.\n",
    "\n",
    "### Bi encoder와 Cross encoder 비교\n",
    "\n",
    "- Bi Decoder는 Pooling을 통해 여러 개 토큰으로 구성된 문장을 하나의 토큰으로 압축. 이러한 방법으로 DB내 모든 문장을 백터화하여 저장 해놓으면 cosine similiarity를 활용해 다양한 Task를 수행할 수 있음. 수십개의 토큰을 하나의 토큰으로 바꾸는 방법이므로 정확도가 낮아지는 단점이 있지만 모든 문장을 하나의 Vector Space 배치하므로 연산속도면에서 장점이 있음\n",
    "\n",
    "- Cross Encoder는 문장을 1:1로 비교해야하는 단점이 있지만 문장 내 모든 토큰을 활용해 연관성을 파악할 수 있으므로 정확도 면에서 장점이 있음.\n",
    "\n",
    "- 하나의 Encoder만 사용하는 경우는 거의 없고 주로 Bi encoder로 우선순위가 높은 문장 50~100개를 추출한 뒤 Cross Encoder를 사용해 상세 순위를 비교하는 방법으로 사용함.\n",
    "\n",
    "<img src='img/Bi_vs_Cross-Encoder.png' alt='comparsion'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ElectraModel, ElectraTokenizerFast\n",
    "\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\n",
    "    \"monologg/koelectra-base-v3-discriminator\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross encoder 구조 살펴보기\n",
    "\n",
    "- Cross encoder는 Model의 Last-hidden-state에 Label 개수에 맞는 output을 반환하는 classifier를 얹은 구조임.\n",
    "\n",
    "- Huggingface의 Sequenceclassification model을 불러오면 쉽게 사용이 가능함.\n",
    "\n",
    "- Sbert에서 제공하는 Cross Encoder도 SequenceClassification 구조를 사용하고 있음.\n",
    "\n",
    "- 따라서 해당 모델의 구조를 살펴봄으로서 Cross Encoder의 구조와 코드 구현 방법에 대해 설명하겠음\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ClassificationHead\n",
    "\n",
    "- Sequence Classification 모델은 Classification Class와 Electra Model Class를 하나로 합친 모델이다.\n",
    "\n",
    "- Classification 구조를 보면 dense_layer => gelu => output_pojection_layer 로 되어있음.\n",
    "\n",
    "- Model의 last hidden layer output 중 [CLS] 토큰만을 활용함.\n",
    "\n",
    "- Model Output은 [batch_size, src_token, embed_size]이 되고 그중 [CLS] 토큰만 활용하므로 [batch_size embed_size]으로 차원이 감소함. 이를 Pooling 한다고 하며 Denselayer와 gelu(Electra에선 gelu, Bert에선 tanh)를 거침 [Why it called pooler? 참고](https://github.com/google-research/bert/issues/1102)\n",
    "\n",
    "- 이후 output_prj_layer를 통해 Label 맞는 차원으로 감소시킴. Regression 모델인 경우 Label size를 1로, Classification 모델인 경우 분류에 필요한 Label 개수에 맞게 설정해야함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import ElectraPreTrainedModel, ElectraForSequenceClassification\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout\n",
    "            if config.classifier_dropout is not None\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.gelu = nn.functional.gelu\n",
    "\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "\n",
    "        # [batch, embed_size] => [batch, num_labels]\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        # [CLS] 토큰 추출 [batch, src_token, embed_size] => [batch, embed_size]\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # label 개수만큼 차원 축소 [batch, embed_size] => [batch, num_labels]\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequenceClassification\n",
    "\n",
    "- ElectraWithClassification은 Electra Model의 Output을 위에서 정의한 Classifier로 연결한 모델임.\n",
    "\n",
    "- 설명을 돕기 위해 ElectraWithClassification을 임의로 만들었으며 ElectraForSequenceClassification 내부 코드를 이해하기 쉽게 일부 변형하였음.\n",
    "\n",
    "- Output은 Label이 모델 내 제공되는 경우(=모델 학습 시) Loss와 Logits으로 반환하고, Label이 제공되지 않는 경우(=평가 시) Logits만 반환함.\n",
    "\n",
    "- Loss function은 학습 유형이 Regression일 때 MSE, Single-Classfication일 때 Cross-Enctropy, Multi-Classification일 때 bcewithlogitsloss를 활용함.\n",
    "  - 학습 유형에 따라 Loss Function이 달라지는 이유에 대해선 [In which cases is the cross-entropy preferred over the mean squared error?](https://stackoverflow.com/questions/36515202/in-which-cases-is-the-cross-entropy-preferred-over-the-mean-squared-error)와 [What is the different between MSE error and Cross-entropy error in NN](https://susanqq.github.io/tmp_post/2017-09-05-crossentropyvsmes/)를 참고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "class ElectraWithClassification(nn.Module):\n",
    "    def __init__(self, model, num_labels) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.config.num_labels = num_labels\n",
    "        self.classifier = classificationHead(self.model.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        # Last-hidden-states 연산\n",
    "        discriminator_hidden_states = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "\n",
    "        # classificationHead에 Last-hidden-state 대입\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Multi-classification 인 경우\n",
    "            # loss_fct = BCEWithLogitsLoss()\n",
    "\n",
    "            # # Regression 인 경우\n",
    "            # loss_fct = MSELoss()\n",
    "            # Single-classfication 인 경우\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "\n",
    "            # _, logits = torch.max(logits, dim=1)\n",
    "            # print(logits.float())\n",
    "            # print(labels.float())\n",
    "            loss = loss_fct(logits.view(-1, 3), labels.view(-1))\n",
    "            return {\"loss\": loss, \"logit\": logits}\n",
    "        else:\n",
    "            return {\"logit\": logits}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorNLI를 활용해 모델 학습 시키겠음\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/KorNLI/snli_1.0_train.ko.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data.columns = [\"sen1\", \"sen2\", \"gold_label\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold_label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "data[\"gold_label\"] = data[\"gold_label\"].replace(label2int).values\n",
    "\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/KorNLI/xnli.dev.ko.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data.columns = [\"sen1\", \"sen2\", \"gold_label\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "data[\"gold_label\"] = data[\"gold_label\"].replace(label2int).values\n",
    "\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "eval_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "eval_data_set[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"gold_label\":\n",
    "                labels.append(int(v))\n",
    "\n",
    "    token = tokenizer(\n",
    "        text_lst1,\n",
    "        text_lst2,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    return dict(**token, labels=torch.LongTensor(labels))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    ")\n",
    "\n",
    "# from transformers import ElectraForSequenceClassification\n",
    "\n",
    "# cross_encoder = ElectraForSequenceClassification.from_pretrained('model/disc_book_final',num_labels=3)\n",
    "# or\n",
    "cross_encoder = ElectraWithClassification(model=model, num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cross_encoder,\n",
    "    train_dataset=train_data_set,\n",
    "    eval_dataset=eval_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# trainer.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
